from urllib.request import urlopen
from bs4 import BeautifulSoup
import pandas as pd
import re

# Scrapes first page of player

# Access web page
html_content = urlopen("http://www.basketball-reference.com/play-index/pgl_finder.cgi?request=1&match=game&year_min=2016&year_max=2016&age_min=0&age_max=99&pos_is_g=Y&pos_is_gf=Y&pos_is_f=Y&pos_is_fg=Y&pos_is_fc=Y&pos_is_c=Y&pos_is_cf=Y&order_by=player&order_by_asc=Y")

# Create html object
soup = BeautifulSoup(html_content, "lxml")

#########################################################################################################################################################

### Manipulating column names

# Accessing the row of the html object that contains the column heading names
column_heading_row = soup.find_all("tr")[0]

# Returns the tags (?)
th_tags = column_heading_row.find_all("th")

# Creates a list containing the column names
col_names=[]
for th in th_tags:
    col_names.append(th.get_text())

col_names_minus_rank = col_names[1:]

# Adds names for missing column names
col_names_minus_rank[5]="Location"
col_names_minus_rank[7]="Result"

##########################################################################################################################################################

### Assigning data in the table into a data frame

# Returns the first table in html_content
table = soup.find_all("table")[0]

# Returns the 2nd (could be any) row in the table
any_row = table.find_all("tr")[3]

# Returns number of columns in table
num_of_columns = len(any_row.find_all('td'))

# Initializes new dataframe
new_df = pd.DataFrame(columns=range(num_of_columns), index=[0])

# Parses through table and puts stats into dataframe going across each row
row_marker = 0
for row in table.find_all('tr'):
    column_marker = 0
    columns = row.find_all('td')
    for column in columns:
        new_df.ix[row_marker,column_marker] = column.get_text()
        column_marker += 1
        if column_marker == num_of_columns:
            row_marker += 1

# Assigns correct column names
new_df.columns=col_names_minus_rank

######################################################################################################################################################

### Loops through and scrapes all remaining pages of player data after first page

i = 100
while len(soup.find_all("table")) > 0:
    
    # Accesses web page
    html_content = urlopen("http://www.basketball-reference.com/play-index/pgl_finder.cgi?request=1&player_id=&match=game&year_min=2016&year_max=2016&age_min=0&age_max=99&team_id=&opp_id=&is_playoffs=&round_id=&game_num_type=&game_num_min=&game_num_max=&game_month=&game_day=&game_location=&game_result=&is_starter=&is_active=&is_hof=&pos_is_g=Y&pos_is_gf=Y&pos_is_f=Y&pos_is_fg=Y&pos_is_fc=Y&pos_is_c=Y&pos_is_cf=Y&c1stat=&c1comp=&c1val=&c2stat=&c2comp=&c2val=&c3stat=&c3comp=&c3val=&c4stat=&c4comp=&c4val=&is_dbl_dbl=&is_trp_dbl=&order_by=player&order_by_asc=Y&offset={}".format(i))

    # Create html object
    soup = BeautifulSoup(html_content, "lxml")

    # Breaks loop upon reaching blank table
    if len(soup.find_all("table")) == 0:
        print("Done")
        break

    # Returns the first table in html_content
    table = soup.find_all("table")[0]
    

    # Parses through table and puts stats into dataframe going across each row
    row_marker = len(new_df)+1
    for row in table.find_all('tr'):
        column_marker = 0
        columns = row.find_all('td')
        for column in columns:
            new_df.ix[row_marker,column_marker] = column.get_text()
            column_marker += 1
            if column_marker == num_of_columns:
                row_marker += 1
    
    # Increments i which increments the page number (100 = 2nd page)
    print(i)
    i += 100

##############################################################################################################

# Writes data set to csv
new_df.to_csv("/Users/Miller/Documents/NBA 2016-17/NBA Player Gamelog Data/2015-2016/2015-2016_nba_player_gamelog_with_playoffs.csv") 

###############################################################################################################
###############################################################################################################

### Merging gamelog with Vegas Odds

# Reads in playoff and regular season data

os.chdir("\\Users\\Miller\\Documents\\NBA 2016-17\\NBA Player Gamelog Data\\2015-2016\\")

reg_and_playoffs=pd.read_csv("2015-2016_nba_player_gamelog_with_playoffs.csv")

# Check each row to see if the Date contains a '*'
reg_and_playoffs['count'] = reg_and_playoffs.apply(lambda reg_and_playoffs: 1 if '*' in reg_and_playoffs.Date else 0, axis = 1)

# Removes asterisk from the end of dates of playoff games
for row in range(len(reg_and_playoffs)):
    if reg_and_playoffs.ix[row,"count"]==1:
        reg_and_playoffs.ix[row,'Date']=str(reg_and_playoffs.ix[row,'Date'])[:-1]
        
#######################################################################################################

# Reads in Vegas Odds Data Set

odds_from_csv=pd.read_csv("/Users/Miller/Documents/NBA 2016-17/Historical NBA Vegas Odds/nba_odds_2015-16__with_game_number.csv")

################################################################################################

### Merges data frame of player data with vegas line data from 2008-2009

gamelog_with_vegas_outer=pd.merge(odds_from_csv, reg_and_playoffs, left_on=["date_formatted","team_formatted"],right_on=["Date", "Tm"], how="outer") #"outer" specifies to retain all observations in left and right dataframes
    
len(gamelog_with_vegas_outer)

gamelog_with_vegas=pd.merge(odds_from_csv, reg_and_playoffs, left_on=["date_formatted","team_formatted"],right_on=["Date", "Tm"], how="inner") #"outer" specifies to retain all observations in left and right dataframes
    
len(gamelog_with_vegas)    


gamelog_with_vegas.to_csv("/Users/Miller/Documents/NBA 2016-17/NBA Player Gamelog Data/2015-2016/2015-2016_nba_player_gamelog_with_vegas_and_game_number.csv")    

##################################################################################################

# Pivoting

new=pd.pivot_table(gamelog_with_vegas, index="Player", columns="Game Number", values=['VH','Team','Close Line','PTS'])

new.to_csv("/Users/Miller/Desktop/test.csv")
